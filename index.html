<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Xiaoqi Zhao</title>
    <meta content="Xiaoqi zhao, https://implus.github.io" name="keywords">
    <style media="screen" type="text/css">
        html,
        body,
        div,
        span,
        applet,
        object,
        iframe,
        h1,
        h2,
        h3,
        h4,
        h5,
        h6,
        p,
        blockquote,
        pre,
        a,
        abbr,
        acronym,
        address,
        big,
        cite,
        code,
        del,
        dfn,
        em,
        font,
        img,
        ins,
        kbd,
        q,
        s,
        samp,
        small,
        strike,
        strong,
        sub,
        tt,
        var,
        dl,
        dt,
        dd,
        ol,
        ul,
        li,
        fieldset,
        form,
        label,
        legend,
        table,
        caption,
        tbody,
        tfoot,
        thead,
        tr,
        th,
        td {
            border: 0pt none;
            font-family: inherit;
            font-size: 100%;
            font-style: inherit;
            font-weight: inherit;
            margin: 0pt;
            outline-color: invert;
            outline-style: none;
            outline-width: 0pt;
            padding: 0pt;
            vertical-align: baseline;
        }

        a {
            color: #1772d0;
            text-decoration: none;
        }

        a:focus,
        a:hover {
            color: #f09228;
            text-decoration: none;
        }

        a.paper {
            font-weight: bold;
            font-size: 12pt;
        }

        b.paper {
            font-weight: bold;
            font-size: 12pt;
        }

        * {
            margin: 0pt;
            padding: 0pt;
        }

        body {
            position: relative;
            margin: 3em auto 2em auto;
            width: 800px;
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 14px;
            background: #eee;
        }

        h2 {
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 15pt;
            font-weight: 700;
        }

        h3 {
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 16px;
            font-weight: 700;
        }

        strong {
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 13px;
            font-weight: bold;
        }

        ul {
            list-style: circle;
        }

        img {
            border: none;
        }

        li {
            padding-bottom: 0.5em;
            margin-left: 1.4em;
        }

        alert {
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 13px;
            font-weight: bold;
            color: #FF0000;
        }

        em,
        i {
            font-style: italic;
        }

        div.section {
            clear: both;
            margin-bottom: 1.5em;
            background: #eee;
        }

        div.spanner {
            clear: both;
        }

        div.paper {
            clear: both;
            margin-top: 0.5em;
            margin-bottom: 1em;
            border: 1px solid #ddd;
            background: #fff;
            padding: 1em 1em 1em 1em;
        }

        div.paper div {
            padding-left: 230px;
        }

        img.paper {
            margin-bottom: 0.5em;
            float: left;
            width: 200px;
        }

        span.blurb {
            font-style: italic;
            display: block;
            margin-top: 0.75em;
            margin-bottom: 0.5em;
        }

        pre,
        code {
            font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
            margin: 1em 0;
            padding: 0;
        }

        div.paper pre {
            font-size: 0.9em;
        }
    </style>

    <link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet"
          type="text/css"/>
    <!-- Global site tag (gtag.js) - Google Analytics -->

</head>


<body>
<div style="margin-bottom: 1em; border: 1px solid #ddd; background-color: #fff; padding: 1em; height: 130px;">
    <div style="margin: 0px auto; width: 100%;">
        <img title="implus" style="float: left; padding-left: .01em; height: 130px;"
             src="./resources/images/3.jpg">
        <div style="padding-left: 12em; vertical-align: top; height: 120px;">
            <span style="line-height: 150%; font-size: 20pt;">Xiaoqi Zhao (赵骁骐)</span><br>
            <span> <a href="https://www.dlut.edu.cn/">Dalian University of Technology (DLUT)</a></span><br> 
            <span> <a href="http://www.gy3000.company/">X3000 INSPECTION - AILab</a></span><br>
            <span><strong>Address</strong>: No.2 Linggong Road, Ganjingzi District, Dalian City, Liaoning, China</span><br>
            <span><strong>Email</strong>: zxq[at]mail.dlut.edu.cn </span> <br>
        </div>
    </div>
</div>
<!--<div style="clear: both; background-color: #fff; margin-top: 1.5em; padding: .2em; padding-left: .3em;">-->

<!--<div style="clear: both;">
    <div class="section">
        <h2>About Me [<a href="https://github.com/Xiaoqi-Zhao-DLUT">GitHub</a>]
            [<a href="https://scholar.google.com/citations?user=0EKcLI4AAAAJ&hl=zh-CN">Google Scholar</a>]
        </h2>
        <div class="paper">
        I received the B.E. degree in electronic and information engineering from the Dalian University of Technology (DLUT), Dalian, China in 2019. I am  currently pursuing the PhD degree in signal and information processing from  DLUT. My advisors are  <a href="http://faculty.dlut.edu.cn/lhzhang/zh_CN/index.htm">Prof. Lihe Zhang</a> and  <a href="http://faculty.dlut.edu.cn/Huchuan_Lu/zh_CN/index.htm">Prof. Huchuan Lu</a> from DLUT.            
 <br><br>
 </div>
</div>-->

<div style="clear: both;">
    <div class="section">
       <h2>About Me [<a href="https://github.com/Xiaoqi-Zhao-DLUT">GitHub</a>]
            [<a href="https://scholar.google.com/citations?user=0EKcLI4AAAAJ&hl=zh-CN">Google Scholar (1800+ citations)</a>]
            <!--[<a href="./resources/cv/wwh_cv.pdf">CV</a>])-->`
        </h2>
        <div class="paper">
            <ul>
                        <!--I received the B.E. degree in electronic and information engineering from the Dalian University of Technology (DLUT), Dalian, China in 2019. I am  currently pursuing the PhD degree in signal and information processing from  DLUT. My advisors are  <a href="http://faculty.dlut.edu.cn/lhzhang/zh_CN/index.htm">Prof. Lihe Zhang</a> and  <a href="http://faculty.dlut.edu.cn/Huchuan_Lu/zh_CN/index.htm">Prof. Huchuan Lu</a> from DLUT.            
 <br><br>-->
                 <li>
                Dalian University of Technology (DLUT), Dalian, China （2015-2019 B.E. degree, postgraduate recommendation top ~10%）
                 </li>
                 <li>
                Dalian University of Technology (DLUT), Dalian, China （2019-current, pursuing the PhD degree, my advisors are  <a href="http://faculty.dlut.edu.cn/lhzhang/zh_CN/index.htm">Prof. Lihe Zhang</a> and  <a href="http://faculty.dlut.edu.cn/Huchuan_Lu/zh_CN/index.htm">Prof. Huchuan Lu (IEEE Fellow)</a> from DLUT）
                 </li>
                 <li>
                   Technology Partner & Chief AI Scientist at <a href="http://www.gy3000.company/">X3000 INSPECTION</a>（pre-A round completed (2023), A round completed (2024)）
                 </li>
                 
 
                 <h2 style="font-family: Georgia; font-size: 1.5rem; color: #2ecc71">Seeking Postdoc Position (after September 2024)</h2>
                 
            </ul>
            <div class="spanner"></div>
        </div>
    </div>
</div>



    
<div style="clear: both;">
    <div class="section">
        <h2 id="confpapers">Research Fields</h2>
        <div class="paper">
            <ul>
                <li>
                 <strong> Context-depend Concept Understanding/Segmentation:</strong> 
               Salient; Camouflaged; Shadow; Transparent 
                   </li>
                   
         <!--        <li>
                 <strong> Natural Scene Image&Video:</strong>  General Object Segmentation (Salient Object Detection, Camouflaged Object Detection, Change Detection); Specific Object Segmentation (Defocus Blur Detection, Shadow Detection, Glass and Transparent Detection, Mirror Detection);  Video Object Segmentation; Crowd Counting 
                   </li> -->
                                     <li>
                 <!--    <strong> X-ray based Complex Industrial Detection:</strong>  Lithium Battery (Winding and lamination); FPCA; Auto Parts   -->
                <strong> Industrial X-ray/CT Machine Vision (Ai4Industury):</strong>  Lithium Battery; FPCA; Auto Parts              </li>
                 <li>
                  <strong> Medical Lesion Segmentation:</strong> Colon Polyp; COVID-19; Breast; Skin
                 </li>
                    <li>
                <strong> Multi-modal Fusion:</strong>  RGB+Depth; RGB+Thermal; RGB+Optical Flow+Depth; RGB+Caption
                       </li>
                 <li>  
                <strong> Learning Mode:</strong>  Multi-task Learning; Unified Model/In-Context Learning; Self-Supervised Learning
                </li>
            </ul>
            <div class="spanner"></div>
        </div>
    </div>
</div>
    

<div style="clear: both;">
    <div class="section">
        <h2 id="confpapers">Honor</h2>
        <div class="paper">
            <ul>
             <alert>  <li>
                  <a href="https://www.vspwdataset.com/Workshop%202023.html">  Third place of PVUW2023 Video Semantic Segmentation Track (CVPR 2023 Challenge) </a>  
           </li>
                 </alert> 
              <alert>  <li>
                     Gold Award of <a href="https://cy.ncss.cn/information/2c93f4c682872dbb01849802948e17dd">The 8th China International College Students' 'Internet+' Innovation and Entrepreneurship Competition</a>, <strong>(Company/Project: 工源三仟; ~0.05‰[161/3,400,000+])</strong>
                 </li>
                 </alert> 
                 <li> 
                  2022 National Scholarship (Phd)
                 </li> 
                
         <alert>  <li>
                  <a href="https://aistudio.baidu.com/aistudio/competition/detail/230/0/leaderboard"> MICCAI 2022 Challenge: Glaucoma Oct Analysis and Layer Segmentation (GOALS)</a>  (Team: IIAU-Segmentors; Glaucoma Detection:<strong> 1/100</strong>; OCT Layer Segmentation:<strong> 3/100</strong>; GOALS:<strong> 2/100</strong>; $1,300 bonus)
           </li>
                 </alert> 
                
                 <alert> 
                 <li> 
                   <a href="https://mp.weixin.qq.com/s/btPSFAqck-_ABQ0YOTsQKQ"> 2022 Huawei Camera Academic Star</a><strong> (top 8 in the world; 30,000 RMB bonus)</strong>
                     </li> 
                 </alert> 
                <alert>  <li>
                   CVPR 2022 Outstanding Reviewer <strong>(~2%[156/6247])</strong>
                </li>
                 </alert> 
                <alert> 
                 <li> 
          <a href="https://www.aminer.cn/conf/ECCV%202020/roster"  style="color:red;text-decoration:none;" >AMiner: Top Chinese Students in ECCV 2020</a>
                </li>    
              </alert> 
                <li>
                    Seventh place of 2020 Hualu Cup's first big data competition (Intelligent Diagnosis of Cancer Risk), <strong>(7/114)</strong>
                </li>
                <li>
                    2020 Huawei Fellowship
                </li>
                 <li>
                    Third place of 2018 OPPO Top AI Competition (Portrait Segmentation), <strong>50,000 RMB bonus (3/456)</strong> 
                </li>
            </ul>
            <div class="spanner"></div>
        </div>
    </div>
</div>

<div style="clear: both;">
    <div class="section">
        <h2 id="experience">News</h2>
        <div class="paper">
            <ul>
            	<li> 1 paper accepted in IJCV 2024.  </li>
                <li> 1 paper accepted in ICCV 2023.  </li>
                <li> 1 paper accepted in IEEE TIP 2023.  </li>
                <li> 1 paper accepted in IEEE TIP 2022.  </li>
              	<li> 1 paper accepted in PRCV 2022.  </li>
                <li> 1 paper accepted in CVPR 2022.  </li>
                <li> 1 paper accepted in AAAI 2022  <alert><strong> (~15%[1349/9020])</strong> </alert> .  </li>
                <li> 1 paper accepted in ACMMM 2021  <alert> <strong>(oral ~9%[179/1942])</strong> </alert> .  </li>
                <li> 1 paper accepted in MICCAI 2021.  </li>
                <li> 3 papers accepted in ECCV 2020  <alert> <strong>(1 oral ~2%[104/5025])</strong> </alert> .  </li>
                <li> 1 paper accepted in CVPR 2020.  </li>
            </ul>
            <div class="spanner"></div>
        </div>
    </div>
</div>

  <div style="clear: both;">
    <div class="section">
        <h2 id="experience">Convenient Tools</h2>
          <div class="paper"><img class="paper" src="./resources/paper_icon/logo_pyseg.png"
                                title="PySegMetrics (PSM): A Python-based Simple yet Efficient Evaluation Toolbox for Segmentation-like tasks">
            <div><strong>PySegMetrics (PSM): A Python-based Simple yet Efficient Evaluation Toolbox for Segmentation-like tasks</strong><br>

                <a href="https://github.com/Xiaoqi-Zhao-DLUT/PySegMetric_EvalToolkit">[Code]</a><img
                        src="https://img.shields.io/github/stars/Xiaoqi-Zhao-DLUT/PySegMetric_EvalToolkit"/>
                        <br>
                <alert> 
                  <br>
                  * Simple yet Efficient.<br>
                  * Rich metrics <br>（F-max, F-mean, weighted F, S-m, E-m, mIoU, mDice, Acc, Ber, MAE）.<br>
                </alert>
            </div>
            <div class="spanner"></divI
        </div>

    </div>
</div>


   
        
        
<div style="clear: both;">
   <div class="section">
        <h2 id="confpapers">Selected Publications  (<a href="https://scholar.google.com/citations?user=0EKcLI4AAAAJ&hl=zh-CN">Full</a>)  <br>
         <h2 style="font-family: Georgia; font-size: 1rem">TPAMI/IJCV/TIP: 7</h2>
         <h2 style="font-family: Georgia; font-size: 1rem">CVPR/ECCV/ICCV/AAAI/MM/MICCAI: 12 (2 oral)</h2>
          </h2> 
        
        
                 <div class="paper"><img class="paper" src="./resources/paper_icon/CVPR24_PBD.png"
                                title="Towards Automatic Power Battery Detection:
New Challenge, Benchmark Dataset and Baseline">
            <div><strong>Towards Automatic Power Battery Detection:
New Challenge, Benchmark Dataset and Baseline</strong><br>
                <strong>Xiaoqi Zhao</strong>, Youwei Pang, Zhenyu Chen, Qian Yu, Lihe Zhang, Hanqi Liu, Jiaming Zuo, Huchuan Lu<br>
                <i>CVPR 2024 (under review)</i><br>
                <a href="https://arxiv.org/pdf/2312.02528.pdf">[Paper]</a>  <br>
                <alert> 
                  * New Challenge: Vision-based power battery detection (Ai4Industury).<br>
                  * New Benchmark: Complex X-ray PBD dataset.<br>
                  * Strong Baseline: Multi-dimensional collaborative network.<br>
                  * Good performance in real-life industury application.<br>
                </alert>
            </div>
            <div class="spanner"></div>
        </div>
        
        
                 <div class="paper"><img class="paper" src="./resources/paper_icon/CVPR24_OVCAMO.png"
                                title="Open-Vocabulary Camouflaged Object Segmentation">
            <div><strong>Open-Vocabulary Camouflaged Object Segmentation</strong><br>
                Youwei Pang, <strong>Xiaoqi Zhao</strong>, Jiaming Zuo, Lihe Zhang, Huchuan Lu<br>
                <i>CVPR 2024 (under review)</i><br>
                <a href="https://arxiv.org/pdf/2311.11241.pdf">[Paper]</a>  <br>
                <alert> 
                  * New Challenge: OVSeg towards camouflaged objects.<br>
                  * New Benchmark: Large-scale OV-Camo dataset.<br>
                  * Strong Baseline: Single stage OVSeg model - OVCoser.<br>
                  * Good performance.<br>
                </alert>
            </div>
            <div class="spanner"></div>
        </div>
        
        
        
      <div class="paper"><img class="paper" src="./resources/paper_icon/IJCV-23-AMP.png"
                                title="Adaptive Multi-source Predictor for Zero-shot Video Object Segmentation">
            <div><strong>Adaptive Multi-source Predictor for Zero-shot Video Object Segmentation</strong><br>
                <strong>Xiaoqi Zhao</strong>, Shijie Chang, Youwei Pang, Jiaxing Yang, <br>
                Lihe Zhang, Huchuan Lu<br>
                 <i>IJCV 2024</i><br>
                <a href="https://arxiv.org/pdf/2303.10383.pdf">[Paper]</a>    <br>
                <alert> * Dual predictors framework towards both static and moving objects.<br>
                  * Multi-source information.<br>
                  * Adaptive predictor fusion.<br>
                  * Predict high-quality depth maps.<br>
                  * Good performance on both ZVOS and RGB-D SOD tasks.<br>
                </alert>
            </div>
            <div class="spanner"></div>
        </div>
  </div>
</div>  

 <div class="paper"><img class="paper" src="./resources/paper_icon/TPAMI24-ZoomNeXt.png"
                                title="ZoomNeXt: A Unified Collaborative Pyramid Network for Camouflaged Object Detection">
            <div><strong>ZoomNeXt: A Unified Collaborative Pyramid Network for Camouflaged Object Detection</strong><br>
                Youwei Pang, <strong>Xiaoqi Zhao</strong>, Tian-Zhu Xiang, Lihe Zhang, Huchuan Lu<br>
                 <i>TPAMI 2024 Submission (major)</i><br>
                <a href="https://arxiv.org/pdf/2310.20208.pdf">[Paper]</a>    <br>
                <alert> 
                  * Unified image and video COD framwrok.<br>
                  * Unifying the scale-specific appearance features at different “zoom” scales.<br>
                  * Uncertainty awareness loss.<br>
                  * Good performance on both image and video COD datasets.<br>
                </alert>
            </div>
            <div class="spanner"></div>
        </div>
  </div>
</div>  

  
           <div class="paper"><img class="paper" src="./resources/paper_icon/IJCV-23-DBS.png"
                                title="Towards Diverse Binary Segmentation via A Simple yet General Gated Network">
            <div><strong>Towards Diverse Binary Segmentation via A Simple yet General Gated Network</strong><br>
                <strong>Xiaoqi Zhao</strong>, Youwei Pang, Lihe Zhang, Huchuan Lu, Lei Zhang <br>
                IJCV Submission (major)<br>
                <a href="https://arxiv.org/pdf/2303.10396.pdf">[Paper]</a>  <br>
                <alert> 
                  * Unified perspective of binary segmentation.<br>
                  * General framework for diverse binary segmentation tasks.<br>
                  * Technique survey for diverse binary segmentation tasks.<br>
                  * Good performance under 10 metrics on 33 datasets of 10 binary segmentation tasks.<br>
                </alert>
            </div>
            <div class="spanner"></div>
        </div>
        
              <div class="paper"><img class="paper" src="./resources/paper_icon/PAMI_Comptr.png"
                                title="ComPtr: Towards Diverse Bi-source Dense Prediction Tasks via A Simple yet General Complementary Transformer">
            <div><strong>ComPtr: Towards Diverse Bi-source Dense Prediction Tasks via A Simple yet General Complementary Transformer</strong><br>
                Youwei Pang*, <strong>Xiaoqi Zhao*</strong>, Lihe Zhang, Huchuan Lu<br>
                IEEE TPAMI Submission (under review) <br>
                <a href="https://arxiv.org/pdf/2307.12349.pdf">[Paper]</a>  <br>
                <alert> 
                  * Unified perspective of bi-source dense prediction tasks.<br>
                  * General framework for diverse bi-source dense prediction tasks.<br>
                  * ComPlementary Transformer.<br>
                  * Good performance in remote sensing change detection, RGB-T crowd counting, RGB-D/T salient object detection, and RGB-D semantic segmentation.<br>
                </alert>
            </div>
            <div class="spanner"></div>
        </div>
        
        
          <div class="paper"><img class="paper" src="./resources/paper_icon/TMI-23-M2SNet.png"
                                title="M2SNet: Multi-scale in Multi-scale Subtraction Network for Medical Image Segmentation">
            <div><strong>M2SNet: Multi-scale in Multi-scale Subtraction Network for Medical Image Segmentation</strong><br>
                <strong>Xiaoqi Zhao</strong>, Hongpeng Jia, Youwei Pang, Long Lv, <br>
                Feng Tian, Lihe Zhang, Weibing Sun, Huchuan Lu<br>
                IEEE TMI Submission (major)<br>
                <a href="https://arxiv.org/pdf/2303.10894.pdf">[Paper]</a>  <br>
                <alert> 
                  * Novel general segmentation architecture.<br>
                  * Efficient intra-layer & inter-layer multi-scale subtraction design.<br>
                  * Low FLOPs (only 9 GFLOPs under the Res2Net-50 backbone). <br>
                  * Good performance on 11 benchmark datasets towards 4 medical segmentations with 4 image modalities (color colonoscopy imaging, ultrasound imaging, computed tomography (CT), and optical coherence tomography (OCT)).<br>
                  * We won the second place (2/100) in the <a href="https://aistudio.baidu.com/aistudio/competition/detail/230/0/leaderboard">MICCAI 2022 Challenge: Glaucoma Oct Analysis and Layer Segmentation (GOALS)</a>.
                </alert>
            </div>
            <div class="spanner"></div>
        </div>
        
        

        

        
                  <div class="paper"><img class="paper" src="./resources/paper_icon/ICCV23-Isomer.png"
                                title="https://arxiv.org/pdf/2303.10383.pdf">
            <div><strong>Isomer: Isomerous Transformer for Zero-shot Video Object Segmentation</strong><br>
                Yichen Yuan, Yifan Wang, Lijun Wang, <strong>Xiaoqi Zhao</strong>, Huchuan Lu, Yu Wang, Weibo Su, Lei Zhang<br>
                in ICCV 2023<br>
                <a href="https://arxiv.org/pdf/2308.06693v1.pdf">[Paper]</a>
                <a href="https://github.com/DLUT-yyc/Isomer">[Code]</a><img
                        src="https://img.shields.io/github/stars/DLUT-yyc/Isomer"/>
                        <br>
                <alert> 
                  * Level-isomerous Transformer.  <br>
                  * Real-time Transformer-based work in the ZVOS field.<br>
                  * Dominant performance on all the ZVOS & VSOD datasets.
                </alert>
            </div>
            <div class="spanner"></div>
        </div>   
        
        
           <div class="paper"><img class="paper" src="./resources/paper_icon/TIP23-CAVER.png"
                                title="https://arxiv.org/pdf/2303.10383.pdf">
            <div><strong>CAVER: Cross-Modal View-Mixed Transformer for Bi-Modal Salient Object Detection</strong><br>
                Youwei Pang, <strong>Xiaoqi Zhao</strong>, Lihe Zhang, Huchuan Lu<br>
                in IEEE TIP 2023<br>
                <a href="https://arxiv.org/pdf/2112.02363.pdf">[Paper]</a>
                <a href="https://github.com/lartpang/CAVER">[Code]</a><img
                        src="https://img.shields.io/github/stars/lartpang/CAVER"/>
                        <br>
                <alert> *  View-mixed attention block.<br>
                  * Patch-wise token re-embedding.<br>
                  * General framework for both RGB-D/T SOD tasks.<br>
                </alert>
            </div>
            <div class="spanner"></div>
        </div>
        

        
           <div class="paper"><img class="paper" src="./resources/paper_icon/TIP2022-MMFT.jpg"
                                title="Joint Learning of Salient Object Detection, Depth Estimation and Contour Extraction">
            <div><strong>Joint Learning of Salient Object Detection, Depth Estimation and Contour Extraction</strong>
                <alert>(I really like this work!)</alert><br>
                <strong>Xiaoqi Zhao</strong>, Youwei Pang, Lihe Zhang, Huchuan Lu<br>
                in IEEE TIP 2022<br>
                <a href="https://arxiv.org/pdf/2203.04895v2.pdf">[Paper]</a>
                <a href="https://github.com/Xiaoqi-Zhao-DLUT/MMFT">[Code]</a><img
                        src="https://img.shields.io/github/stars/Xiaoqi-Zhao-DLUT/MMFT"/>
                        <br>
                <alert> *  Multi-task Learning.<br>
                  * Attention-in-Attention (Transformer + Dynamic filter).<br>
                  * High-quality depth predictor supervised by low-quaility depth maps.<br>
                  * Good performance on all three tasks.<br>
                  * Thorough ablation studies on all three tasks.<br>
                  * We can provide AI-assisted bokeh with the extreme mode and progressive mode.<br>
                  * We update many RGB-D SOD datasets with the MMFT predicted high-quality maps. Many top-performing methods obtain a considerable performance gain.
                </alert>
            </div>
            <div class="spanner"></div>
        </div>
        
  <div class="paper"><img class="paper" src="./resources/paper_icon/CVPR2022-ZoomNet.png"
                                title="Zoom In and Out: A Mixed-scale Triplet Network for Camouflaged Object Detection">
            <div><strong>Zoom In and Out: A Mixed-scale Triplet Network for Camouflaged Object Detection</strong><br>
               Youwei Pang*, <strong>Xiaoqi Zhao</strong>*, Tian-zhu Xiang, Lihe Zhang, Huchuan Lu<br>
                in CVPR 2022<br>
                <a href="https://arxiv.org/pdf/2203.02688.pdf">[Paper]</a>
                <a href="https://github.com/lartpang/ZoomNet">[Code]</a><img
                        src="https://img.shields.io/github/stars/lartpang/ZoomNet"/>
                        <br>
                <alert> *  A mixed-scale  triplet network.<br>
                  * The scale integration unit (SIU) and the hierarchical mixed-scale unit (HMU).<br>
                  * A uncertainty-aware loss (UAL).<br>
                  * Good performance on both COD and SOD datasets.
                </alert>
            </div>
            <div class="spanner"></div>
        </div>


 
   <div class="paper"><img class="paper" src="./resources/paper_icon/2021-SSL.png"
                                title="Self-Supervised Pretraining for RGB-D Salient Object Detection">
            <div><strong>Self-Supervised Pretraining for RGB-D Salient Object Detection</strong><br>
                <strong>Xiaoqi Zhao</strong>, Youwei Pang, Lihe Zhang, Huchuan Lu, Xiang Ruan<br>
                in AAAI 2022<br>
                <a href="https://arxiv.org/pdf/2101.12482.pdf">[Paper]</a>
              <a href="https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247591348&idx=2&sn=34a95dbbd14b7adfd508f12899ac27a2&chksm=ec1d884ddb6a015b71612bf16227bed8c69db4074585f712c1dc7dceb7859641088441ff9753&mpshare=1&scene=1&srcid=0112O4ivFbyChzTTH4vKU91t&sharer_sharetime=1646716025907&sharer_shareid=0ffc6ac03af605267e92344350efdb83&exportkey=AxNWDL6LKSsLK6MxSNkSF88%3D&acctmode=0&pass_ticket=TXKuWY6yeluRhUKTt0pk10ycuy%2BMsyJV6%2BXdxFjTtusuYyJMVPywg38icEXhKktM&wx_header=0#rd">[Slide&极市平台推送]</a>
                <a href="https://github.com/Xiaoqi-Zhao-DLUT/SSLSOD">[Code]</a><img
                        src="https://img.shields.io/github/stars/Xiaoqi-Zhao-DLUT/SSLSOD"/>
                        <br>
                <alert> 
                  * The first one self-supervised pretraining method for RGB-D SOD. <br>
                  * Pretext tasks: a cross-modal auto-encoder and a depth-contour estimation decoder.  <br>
                  *  The consistency-difference aggregation structure that is suitable for both cross-level and cross-modal feature integration.<br>
                  * Pretraining datasets: 6,392 pairs of RGB-Depth images (without manual  annotations)  vs. ImageNet (1,280,000 with image-level labels). <br>
                  * Good performance on RGB-D SOD datasets.
                </alert>
            </div>
            <div class="spanner"></div>
        </div>


 <div class="paper"><img class="paper" src="./resources/paper_icon/ACMMM2021-Multisource.png"
                                title="Multi-Source Fusion and Automatic Predictor Selection for Zero-Shot Video Object Segmentation">
            <div><strong>Multi-Source Fusion and Automatic Predictor Selection for Zero-Shot Video Object Segmentation</strong><br>
                <strong>Xiaoqi Zhao</strong>, Youwei Pang, Jiaxing Yang, Lihe Zhang, Huchuan Lu<br>
     in ACMMM 2021 (<alert>Oral
                </alert>)<br>
                <a href="https://arxiv.org/pdf/2108.05076.pdf">[Paper]</a>
                <a href="https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247570266&idx=2&sn=547fbfdeaa02b0ff83dec3b24a58a632&chksm=ec1d3ea3db6ab7b5cfbe7ba2fffcd4460584f1e2743d8265562757e508030bf049d1c3140d3c&mpshare=1&scene=23&srcid=0902BUt8U9MY8k2IvY500f71&sharer_sharetime=1630565724193&sharer_shareid=5c55e87df338791d997e8905ad2ebfe0#rd">[Slide&极市平台推送]</a>
                <a href="https://github.com/Xiaoqi-Zhao-DLUT/Multi-Source-APS-ZVOS">[Code]</a><img
                        src="https://img.shields.io/github/stars/Xiaoqi-Zhao-DLUT/Multi-Source-APS-ZVOS"/>
                <br>
                <alert> 
                  * The first one utilizes multi-source information to achieve static/moving object segmentation.<br>
                  * The first one aims to evaluate the quality of optical flow for segmentation tasks.<br>
                  * Multiple sources (rgb + depth + optical flow + static saliency) for ZVOS.<br>
                  * Interoceptive spatial attention module (ISAM).<br>
                  * Feature purification module (FPM).<br>
                  * Automatic predictor selection network.<br>
                                   
                </alert>
            </div>
            <div class="spanner"></div>
        </div>
        
 <div class="paper"><img class="paper" src="./resources/paper_icon/MICCAI2021-MSNet.png"
                                title="Automatic Polyp Segmentation via　 Multi-scale Subtraction Network">
            <div><strong>Automatic Polyp Segmentation via Multi-scale Subtraction Network</strong><br>
                <strong>Xiaoqi Zhao</strong>,  Lihe Zhang, Huchuan Lu<br>
     in MICCAI 2021 <br>
                 <a href="https://arxiv.org/pdf/2108.05082.pdf">[Paper]</a>
                <a href="https://github.com/Xiaoqi-Zhao-DLUT/MSNet">[Code]</a><img
                        src="https://img.shields.io/github/stars/Xiaoqi-Zhao-DLUT/MSNet"/>
                <br>
                <alert> 
                  * Automatic  polyp segmentation.<br>
                  * Fast speed (∼70fps).<br>
                  * A multi-scale  subtraction  network.<br>
                  * A training-free  loss  network.<br>
                </alert>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/paper_icon/ECCV2020-GateNet.png"
                                title="Suppress and Balance: A Simple Gated Network for Salient Object Detection">
            <div><strong>Suppress and Balance: A Simple Gated Network for Salient Object Detection</strong><br>
                <strong>Xiaoqi Zhao</strong>*, Youwei Pang*, Lihe Zhang, Huchuan Lu, Lei Zhang<br>
                in ECCV 2020 (<alert>Oral
                </alert>)<br>
                <a href="https://arxiv.org/pdf/2007.08074.pdf">[Paper]</a>
                <a href=" https://github.com/Xiaoqi-Zhao-DLUT/GateNet-RGB-Saliency/blob/master/2852.pdf">[Slide]</a>
                <a href="https://github.com/Xiaoqi-Zhao-DLUT/GateNet-RGB-Saliency">[Code]</a><img
                        src="https://img.shields.io/github/stars/Xiaoqi-Zhao-DLUT/GateNet-RGB-Saliency"/>
              
                <br>
                <alert> 
                  * Simple & Effective.<br>
                  * A new baseline -- GateNet.<br>
                  * A new convolution -- Fold conv.<br>
                  * A new style -- Progressive + Prallel.<br>
                  * Good performance on RGB, RGB-D SOD and video object segmentaton datasets.
                </alert>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/paper_icon/ECCV2020-DANet.png"
                                title="A Single Stream Network for Robust and Real-time RGB-D Salient Object Detection">
            <div><strong>A Single Stream Network for Robust and Real-time RGB-D Salient Object Detection</strong><br>
               <strong>Xiaoqi Zhao</strong>,  Lihe Zhang, Youwei Pang, Huchuan Lu, Lei Zhang<br>
                in ECCV 2020 <br>
                <a href="https://arxiv.org/pdf/2007.06811.pdf">[Paper]</a>
                <a href="https://github.com/Xiaoqi-Zhao-DLUT/DANet-RGBD-Saliency">[Code]</a><img
                        src="https://img.shields.io/github/stars/Xiaoqi-Zhao-DLUT/DANet-RGBD-Saliency"/>
       
                <br>
                <alert> 
                  * The first one single stream network for RGB-D SOD.<br>
                  * Light (55.5% lighter than the second lightest model) & Fastest speed (32 FPS).<br>
                  * Depth-enhanced dual attention mechanism.<br>
                  * Pyramidally  attended  feature  extraction  module.<br>
                  * Good performance on RGB-D SOD datasets.<br>
                </alert>            </div>
            <div class="spanner"></div>
        </div>

 <div class="paper"><img class="paper" src="./resources/paper_icon/CVPR2021-MINet.png"
                                title="Multi-scale Interactive Network for Salient Object Detection">
            <div><strong>Multi-scale Interactive Network for Salient Object Detection</strong><br>
                 Youwei Pang*, <strong>Xiaoqi Zhao</strong>*, Lihe Zhang, Huchuan Lu<br>
                in CVPR 2020<br>
                [<a href="https://arxiv.org/pdf/2007.09062.pdf">Paper</a>]
                [<a href="https://github.com/lartpang/MINet">Code</a>]<img src="https://img.shields.io/github/stars/lartpang/MINet"/>
   
                <br>
                <alert>
                  * Multi-scale.<br>
                  * Aggregate Interaction Module.<br>
                  * Self-Interaction Module.<br>
                  * Consistency-enhanced loss. <br>
                  * Good performance on RGB SOD datasets.<br>
                </alert>
            </div>
            <div class="spanner"></div>
        </div>

<div class="paper"><img class="paper" src="./resources/paper_icon/ECCV2020-HDFNet.png"
                                title="Hierarchical Dynamic Filtering Network for RGB-D Salient Object Detection">
            <div><strong>Hierarchical Dynamic Filtering Network for RGB-D Salient Object Detection</strong><br>
                 Youwei Pang, Lihe Zhang, <strong>Xiaoqi Zhao</strong>,  Huchuan Lu<br>
                in ECCV 2020<br>
                [<a href="https://arxiv.org/pdf/2007.06227.pdf">Paper</a>]
                [<a href="https://github.com/lartpang/HDFNet/blob/master/LongPresentation.pdf">Slide</a>]
                [<a href="https://github.com/lartpang/HDFNet">Code</a>]<img src="https://img.shields.io/github/stars/lartpang/HDFNet"/>
         
                <br>
                <alert>
                  * The first dynamic conv-based network for RGB-D SOD.<br>
                  * Hybrid enhanced loss.<br>
                  * Good performance on RGB-D SOD datasets.<br>
                  * The proposed model (HDFNet) is an important baseline of the
                [<a href="https://arxiv.org/pdf/2105.00690.pdf">winning solution</a>] in NTIRE 2021 (Depth Guided Image Relighting Challenge) hosted in CVPR 2021 workshop (winner: AICSNTU-MBNet team (Asustek Computer Inc & National Taiwan University)).<br>
                </alert>
            </div>
            <div class="spanner"></div>
        </div>


    </div>
</div>



<div style="clear: both;">
    <div class="section">
        <h2>Review Services</h2>
        <div class="paper">
            <strong>Journal Reviewer</strong><br>
            T-PAMI, IJCV, TIP, TMI
            <br>

            <strong>Conference Reviewer</strong><br>
            CVPR: 2023, 2022 <strong>(Outstanding Reviewer ~2%[156/6247])</strong><br>
            ICCV: 2023, 2021 <br>
          	ECCV: 2022 <br>
            AAAI: 2024, 2023, 2022 <br>
            NeurIPS: 2023 <br>
            ICLR: 2024 <br>
            ICML: 2024 <br>
            <br>
        </div>
    </div>
<!--<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com%%%/map_v2.js?d=qF0yb8INQ6Byeh4ZPe-hmtAT1XxBhyDE0-o194UrGx8&cl=ffffff&w=a"></script>-->
</body>
</html>
<script type="text/javascript" src="//rf.revolvermaps.com/0/0/6.js?i=5267tz04tuz&amp;m=7&amp;c=e63100&amp;cr1=ffffff&amp;f=arial&amp;l=0&amp;bv=90&amp;lx=-420&amp;ly=420&amp;hi=20&amp;he=7&amp;hc=a8ddff&amp;rs=80" async="async"></script>
