<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<link rel="shortcut icon" href="myIcon.ico">
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css">
<title>Xiaoqi Zhao &#39;s Homepage</title>
<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-159069803-1', 'auto');
ga('send', 'pageview');
</script>
<!-- End Google Analytics -->
<!--
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-87320911-1', 'auto');
  ga('send', 'pageview');

</script>
-->
</head>
<body>
<div id="layout-content" style="margin-top:25px">
<table>
	<tbody>
		<tr>
			<td width="670">
				<div id="toptitle">
					<h1>Xiaoqi Zhao <font face="Arial">    ËµµÈ™ÅÈ™ê </font></h1></div>

		
				<h3> <a href="https://futureschool.dlut.edu.cn/IIAU.htm">Dalian University of Technology - IIAU LAB</a><br> </h3>
<!-- 				<h3> <a href="http://www.gy3000.company/">X3000 INSPECTION - AI LAB</a><br></h3> -->
				<span><strong>Address</strong>: No.2 Linggong Road, Ganjingzi District, Dalian City, Liaoning, China</span><br>
				<span><strong>Email</strong>: zxq[at]mail.dlut.edu.cn </span> <br>
				<br>
				[<a href="https://github.com/Xiaoqi-Zhao-DLUT">GitHub</a>]
				[<a href="https://scholar.google.com/citations?user=0EKcLI4AAAAJ&hl=zh-CN">Google Scholar</a>]
			</td>
			<td>
				<img src="./resources/images/jap.jpg" border="0" width="220"><br>
			</td>
		</tr><tr>
	</tr></tbody>
</table>

<h2>Biography </h2>
<p>
<!-- 		 <li>
		   Technology Partner & Chief AI Scientist at <a href="http://www.gy3000.company/">X3000 INSPECTION</a>ÔºàSeries A+ RoundÔºâ
		 </li> -->
		 <li>
		Dalian University of Technology (DLUT), Dalian, China Ôºà2019-2024 PhD degree, my advisors are  <a href="http://faculty.dlut.edu.cn/lhzhang/zh_CN/index.htm">Prof. Lihe Zhang</a> and  <a href="http://faculty.dlut.edu.cn/Huchuan_Lu/zh_CN/index.htm">Prof. Huchuan Lu (IEEE Fellow)</a> from <a href="https://futureschool.dlut.edu.cn/IIAU.htm">IIAU-LAB.</a> I work very closely with my best friend <a href="https://lartpang.github.io/">Youwei Pang.</a>Ôºâ
		 </li>
	<li>
		Dalian University of Technology (DLUT), Dalian, China Ôºà2015-2019 B.E. degreeÔºâ
		 </li>
	
	

</p>


<div style="clear: both;">
    <div class="section">
        <h2 id="confpapers">Research Fields</h2>
        <div class="paper">
            <ul>
                <li>
                 <strong> Unified Context-dependent Concept Segmentation (Unified Vision Model)</strong> <br>
               Salient + Camouflaged + Shadow + Transparent + Inharmonious + Defocus Blur + Medical Lesion  ... 
                   </li>
                   
         <!--        <li>
                 <strong> Natural Scene Image&Video:</strong>  General Object Segmentation (Salient Object Detection, Camouflaged Object Detection, Change Detection); Specific Object Segmentation (Defocus Blur Detection, Shadow Detection, Glass and Transparent Detection, Mirror Detection);  Video Object Segmentation; Crowd Counting 
                   </li> -->
                                     <li>
                 <!--    <strong> X-ray based Complex Industrial Detection</strong>  
					Lithium Battery (Winding and lamination); FPCA; Auto Parts   -->
                <strong> Industrial X-ray/CT Machine Vision (Ai4Industry)</strong><br>  
                                       Lithium Battery; FPCA; Auto Parts              </li>
		    <li>
                  <strong>Medical Lesion Segmentation & Device Imaging Quality Enhancement (Ai4Health)</strong><br> 
                   Colon Polyp; COVID-19; Breast; Skin; Glaucoma
                 </li>
                 <li><strong> Self-driven Learning Mode</strong><br> 
                   Self-supervised/Few-shot/In-context/Prompt/Open-Vocabulary Learning
                </li>
                    <li>
                <strong> Multi-modal/source/view Fusion</strong><br> 
                    RGB + X (Depth; Thermal; Optical Flow; Text; Event; Video; View)
                       </li> 
            </ul>
            <div class="spanner"></div>
        </div>
    </div>
</div>

<div style="clear: both;">
    <div class="section">
        <h2 id="confpapers">Honor</h2>
        <div class="paper">
            <ul>
		      <alert> 
                 <li> 
                   <a href="https://mp.weixin.qq.com/s/btPSFAqck-_ABQ0YOTsQKQ"> 2022 Huawei Camera Academic Star</a><strong> (top 8 in the world; 30,000 RMB bonus)</strong>
                     </li> 
                 </alert>
		<alert>  <li>
                   CVPR 2022 Outstanding Reviewer <strong>(~2%[156/6247])</strong>
                </li>
                 </alert> 
             <alert>  <li>
                  <a href="https://www.vspwdataset.com/Workshop%202023.html">  Third place of PVUW2023 Video Semantic Segmentation Track (CVPR 2023 Challenge) </a>  
           </li>
                 </alert> 
              <alert>  <li>
                     Gold Award of <a href="https://cy.ncss.cn/information/2c93f4c682872dbb01849802948e17dd">The 8th China International College Students' 'Internet+' Innovation and Entrepreneurship Competition</a>, <strong>(Company/Project: Â∑•Ê∫ê‰∏â‰ªü; ~0.05‚Ä∞[161/3,400,000+])</strong>
                 </li>
                 </alert> 
         <alert>  <li>
                  <a href="https://aistudio.baidu.com/aistudio/competition/detail/230/0/leaderboard"> MICCAI 2022 Challenge: Glaucoma Oct Analysis and Layer Segmentation (GOALS)</a>  (Team: IIAU-Segmentors; Glaucoma Detection:<strong> 1/100</strong>; OCT Layer Segmentation:<strong> 3/100</strong>; GOALS:<strong> 2/100</strong>; $1,300 bonus)
           </li>
                 </alert> 
		<li>
                    Third place of 2018 OPPO Top AI Competition (Portrait Segmentation), <strong>50,000 RMB bonus (3/456)</strong> 
                </li>
                <li>
                    Seventh place of 2020 Hualu Cup's first big data competition (Intelligent Diagnosis of Cancer Risk), <strong>(7/114)</strong>
                </li>
		  <li>Second Prize of Doctoral Academic Forum (2024), Supported by Dalian University of Technology School
			  </li>
	  <li>  2024 National Scholarship (Phd) </li>
		   <li>  2023 AVIC-LuoYang Scholarship (Phd)
	  </li>
   <li>  2022 National Scholarship (Phd) </li>
<li>   2020 Huawei Fellowship </li>
  <li>  2019-2024 Outstanding graduate student of Dalian University of Technology </li>
            </ul>
            <div class="spanner"></div>
        </div>
    </div>
</div>

<h2 id="experience">News</h2>
<div style="height: 280px; overflow: auto;">
    <div class="section">   
        <div class="paper">
            <ul>
		      <li>[02/2025]1 paper accepted in CVPR 2025. Congratulations to <a href="https://scholar.google.com/citations?user=yn5W75MAAAAJ&hl=zh-CN">Shijie</a> ! </li>
		    <li>[01/2025] Our CAVER (IEEE TIP 2023) is the <alert><strong> ESI Highly Cited Paper (1%)</strong> </alert>.</li>
		<li>[12/2024] I successfully defended my PhD dissertation! Call me Dr.Zhao!üòéüòéüòé</li>
		<li>[08/2024] <a href="https://mp.weixin.qq.com/s/t7zYXtV-M6cgGUTyvRkpaw">X3000 INSPECTION fininshed Series A and A+ financing rounds</a>. üéâüéâüéâ Ai4EVSafety is all you needÔºÅ    </li>
		<li>[07/2024] 1 paper accepted in ECCV 2024.  <alert><strong>New Task! Open-Vocabulary Camouflaged Object Segmentation</strong> </alert> .    </li>
		    <li>[06/2024] 1 paper accepted in TPAMI 2024.   </li>
          	<li>[05/2024] 1 paper accepted in ICML 2024.   </li>
				<li>[03/2024] 1 paper accepted in IJCV 2024.   </li>
		         <li>[02/2024] 2 papers accepted in CVPR 2024 <alert> <strong>(1 highlight ~3%[324/11532])</strong> </alert>. <strong> Ai4Industry in New Electrical Vehicle! </strong>   </li>
            	<li> [01/2024] 1 paper accepted in IJCV 2024.  </li>
                <li> [07/2023] 1 paper accepted in ICCV 2023.  </li>
                <li> [01/2023] 1 paper accepted in IEEE TIP 2023.  </li>
                <li> [12/2022] 1 paper accepted in IEEE TIP 2022.  </li>
                <li> [03/2022] 1 paper accepted in CVPR 2022.  </li>
                <li> [11/2021] 1 paper accepted in AAAI 2022  <alert><strong> (~15%[1349/9020])</strong> </alert> .  </li>
                <li> [07/2021] 1 paper accepted in ACM MM 2021  <alert> <strong>(oral ~9%[179/1942])</strong> </alert> .  </li>
                <li> [06/2021] 1 paper accepted in MICCAI 2021.  </li>
                <li> [07/2020] 3 papers accepted in ECCV 2020  <alert> <strong>(1 oral ~2%[104/5025])</strong> </alert> .  </li>
                <li> [03/2020] 1 paper accepted in CVPR 2020.  </li>
            </ul>
            <div class="spanner"></div>
        </div>
    </div>
</div>

<!-- <span style="color:red; white-space: nowrap;">(One Spotlight)</span>. -->





<h2> Selected Publications   [<a href="https://scholar.google.com/citations?user=0EKcLI4AAAAJ&hl=zh-CN">Google Scholar (3000 + citations)</a>]</h2>
	<!-- <br>      <span style="color:red; white-space: nowrap;"><b>(* indicates equal contribution)</b></span> -->
	<br> <b>TPAMI/IJCV/TIP: 7</b>
	<br> <b>ICML/CVPR/ECCV/ICCV/AAAI/MM/MICCAI: 14 (2 oral, 1 highlight)</b>
	<br> <b>Papers with 500+ Citations: 2</b>
	<br> <b>Papers with 100+ Citations: 8</b>
	<br> <b>Papers as First/co-First Author: 17</b>
	<br>


	<tr><tr><tr><tr>
		<div style="margin-top: 10px"></div>
			<!-- <h2>Selected Publications <a href="https://scholar.google.com/citations?hl=en&user=botczdcAAAAJ">(Google Scholar)</a> </h2> -->
		<!--<p><a href="https://scholar.google.com/citations?hl=en&user=botczdcAAAAJ">My Google Scholar</a></p>-->
			 
		<!--<table id="tbPublications" width="100%">
			<tbody>
		<h3 style="color: red">Preprints</h3>
				
		   <tr>	
				<td width="206">
				<img src="img/TransZero-TPAMI.png" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td><a href="TransZero-pp/TransZero-pp.html" target="_blank">TransZero++: Cross Attribute-guided Transformer for Zero-Shot Learning.</a><br>
				<p ><strong>Shiming Chen</strong>, Ziming Hong, Guo-Sen Xie, Jian Zhao, Xinge You, Shuicheng Yan, Ling Shao.</p>
				<p class="post-date" style="margin-top: -10px" ><i>arXiv preprint arXiv: 2111.04254</i>, 2021. <strong>
				[<a href="TransZero-pp/TransZero-pp.html">Project Page</a>][<a href="http://arxiv.org/abs/2112.08643">arXiv</a>]
				[<a href="https://github.com/shiming-chen/TransZero_pp">Code</a>]</strong>
				</p>		
			 <p style="margin-top: -11px"><span style="color:Green">Submitted to <i>IEEE Transactions Pattern Analysis and Machine Intelligence (<strong> TPAMI </strong>)</i> (Minor Revision)</span></p>
				</td>
			</tr>
			<tr></tr>
			<tr></tr>
			
		 </tbody>
		</table>-->	      
		<table id="tbPublications" width="100%">
		<tbody>
		<!-- <span>(*:Co-First Author) </span> -->
		<h3 style="color: red">Conference Papers </h3>	

		<tr>
			
				<td width="206">
				<img src="./resources/paper_icon/SAM-EVA2025.png" width="185px" height = "120px" style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td> <a href="https://arxiv.org/pdf/2412.01240" target="_blank"> Inspiring the Next Generation of Segment Anything Models: Comprehensively Evaluate SAM and SAM 2 with Diverse Prompts Towards Context-Dependent Concepts under Different Scenes</a>
					<a href="https://arxiv.org/pdf/2412.01240">[Paper]</a>  
					<a href="https://github.com/lartpang/SAMs-CDConcepts-Eval">[Code]</a><img
								src="https://img.shields.io/github/stars/lartpang/SAMs-CDConcepts-Eval"/>
				 <p ><strong>Xiaoqi Zhao*</strong>, Youwei Pang*, Shijie Chang*, Yuan Zhao*, Lihe Zhang, Huchuan Lu, Jinsong Ouyang, Georges El Fakhri, Xiaofeng Liu</p>
			   <p style="margin-top: -11px"><i>arXiv, 2025. </i></p>
				</td>
			</tr>
			<tr></tr>
			<tr></tr>
			
      		<tr>
			
				<td width="206">
				<img src="./resources/paper_icon/Spider-ICML2024.png" width="185px" height = "120px" style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td> <a href="https://arxiv.org/pdf/2405.01002.pdf" target="_blank"> Spider: A Unified Framework for Context-dependent Concept Segmentation</a>
					<a href="https://arxiv.org/pdf/2405.01002.pdf">[Paper]</a>  
					<a href="https://mp.weixin.qq.com/s/_qaKf6ctRm0A0VYWL63umQ">[Interpretation by CVHub (~7000 views)]</a>
					<a href="https://github.com/Xiaoqi-Zhao-DLUT/Spider-UniCDSeg">[Code]</a><img
								src="https://img.shields.io/github/stars/Xiaoqi-Zhao-DLUT/Spider-UniCDSeg"/>
				 <p ><strong>Xiaoqi Zhao*</strong>, Youweii Pang*, Wei Ji*, Baicheng Sheng, Jiaming Zuo, Lihe Zhang, Huchuan Lu</p>
			   <p style="margin-top: -11px"><i>International Conference on Machine Learning(<strong> ICML </strong>), 2024. </i></p>
				</td>
			</tr>
			<tr></tr>
			<tr></tr>	
          
		<tr>
			
				<td width="206">
				<img src="./resources/paper_icon/CVPR24-PBD-1.png" width="185px" height = "120px" style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td> <a href="https://arxiv.org/pdf/2312.02528v2.pdf" target="_blank"> Towards Automatic Power Battery Detection:
					New Challenge, Benchmark Dataset and Baseline</a>
					<a href="https://arxiv.org/pdf/2312.02528v2.pdf">[Paper]</a>  
					<a href="https://mp.weixin.qq.com/s/ZGI3aGQqOFCzCVWETKe4VA">[Interpretation by ÊûÅÂ∏ÇÂπ≥Âè∞]</a>
					<a href="https://mp.weixin.qq.com/s/7YIxzGnwimo7UOb-Nw62mw">[Interpretation by X3000 INSPECTION]</a>
					<a href="https://github.com/Xiaoqi-Zhao-DLUT/X-ray-PBD">[Code]</a><img
								src="https://img.shields.io/github/stars/Xiaoqi-Zhao-DLUT/X-ray-PBD"/>
				 <p ><strong>Xiaoqi Zhao*</strong>, Youwei Pang*, Zhenyu Chen, Qian Yu, Lihe Zhang, Hanqi Liu, Jiaming Zuo, Huchuan Lu</p>
			   <p style="margin-top: -11px"><i>IEEE Conference on Computer Vision and Pattern Recognition (<strong> CVPR </strong>), 2024. </i></p>
				</td>
			</tr>
			<tr></tr>
			<tr></tr>	
		
			<tr>
			
				<td width="206">
				<img src="./resources/paper_icon/CVPR24-MVNet.png" width="185px" height = "90px" style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td> <a href="" target="_blank"> Multi-view Aggregation Network for Dichotomous Image Segmentation</a>
				<a href="https://arxiv.org/pdf/2404.07445.pdf">[Paper]</a>  
					<a href="https://github.com/qianyu-dlut/MVANet">[Code]</a><img
								src="https://img.shields.io/github/stars/qianyu-dlut/MVANet"/>
				 <p >Qian Yu*, <strong>Xiaoqi Zhao*</strong>, Youwei Pang*, Lihe Zhang, Huchuan Lu</p>
			   <p style="margin-top: -11px"><i>IEEE Conference on Computer Vision and Pattern Recognition (<strong> CVPR </strong>), 2024. 
					<a  style="color:red;text-decoration:none;" ><strong>Highlight</strong</a></i></p>
				</td>
			</tr>
			<tr></tr>
			<tr></tr>	
		
			<tr> 
			
				<td width="206">
				<img src="./resources/paper_icon/CVPR24-OVCAMO.png" width="185px" height = "90" style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td> <a href="https://arxiv.org/pdf/2311.11241.pdf" target="_blank"> Open-Vocabulary Camouflaged Object Segmentation</a>
					<!-- <p ><a href="https://arxiv.org/pdf/2311.11241.pdf">[Paper]</a>  </p> -->
				<a href="https://arxiv.org/pdf/2311.11241">[Paper]</a>  
					<a href="https://mp.weixin.qq.com/s/JdhHr5-X1578STcTaDcg3Q">[Interpretation by CVHub (~7000 views)]</a>
					<a href="https://mp.weixin.qq.com/s/7Ero0b8v5O4dqfcTJD_oJw">[Interpretation by ÊûÅÂ∏ÇÂπ≥Âè∞]</a>
					<a href="https://github.com/lartpang/OVCamo">[Code]</a><img
								src="https://img.shields.io/github/stars/lartpang/OVCamo"/>
				 <p >Youwei Pang*, <strong>Xiaoqi Zhao*</strong>, Jiaming Zuo, Lihe Zhang, Huchuan Lu</p>
			   <p style="margin-top: -11px"><i>European Conference on Computer Vision (<strong> ECCV </strong>), 2024. </i></p>
				</td>
			</tr>
			<tr></tr>
			<tr></tr>	
			
			<tr>
			
				<td width="206">
				<img src="./resources/paper_icon/ICCV23-Isomer.png" width="185px" height = "90" style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td> <a href="https://arxiv.org/pdf/2308.06693v1.pdf" target="_blank"> Isomer: Isomerous Transformer for Zero-shot Video Object Segmentation</a>
					<a href="https://arxiv.org/pdf/2308.06693v1.pdf">[Paper]</a>  
					<a href="https://github.com/DLUT-yyc/Isomer">[Code]</a><img
								src="https://img.shields.io/github/stars/DLUT-yyc/Isomer"/>
				 <p >Yichen Yuan, Yifan Wang, Lijun Wang, <strong>Xiaoqi Zhao</strong>, Huchuan Lu, Yu Wang, Weibo Su, Lei Zhang</p>
			   <p style="margin-top: -11px"><i>IEEE International Conference on Computer Vision (<strong> ICCV </strong>), 2023. </i></p>
				</td>
			</tr>
			<tr></tr>
			<tr></tr>	
			<tr>
			
				<td width="206">
				<img src="./resources/paper_icon/CVPR2022-ZoomNet.png" width="185px" height = "90" style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td> <a href="https://arxiv.org/pdf/2203.02688.pdf" target="_blank"> Zoom In and Out: A Mixed-scale Triplet Network for Camouflaged Object Detection</a>
					<a href="https://arxiv.org/pdf/2203.02688.pdf">[Paper]</a>  
					<a href="https://github.com/lartpang/ZoomNet">[Code]</a><img
								src="https://img.shields.io/github/stars/lartpang/ZoomNet"/>
				 <p >  Youwei Pang*, <strong>Xiaoqi Zhao</strong>*, Tian-zhu Xiang, Lihe Zhang, Huchuan Lu</p>
			   <p style="margin-top: -11px"><i>IEEE Conference on Computer Vision and Pattern Recognition (<strong> CVPR </strong>), 2022. </i></p>
				</td>
			</tr>
			<tr></tr>
			<tr></tr>	
		
			<tr>
			
				<td width="206">
				<img src="./resources/paper_icon/2021-SSL.png" width="185px" height = "90" style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td> <a href="https://arxiv.org/pdf/2101.12482.pdf" target="_blank"> Self-Supervised Pretraining for RGB-D Salient Object Detection</a>
					<a href="https://arxiv.org/pdf/2101.12482.pdf">[Paper]</a>
					<a href="https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247591348&idx=2&sn=34a95dbbd14b7adfd508f12899ac27a2&chksm=ec1d884ddb6a015b71612bf16227bed8c69db4074585f712c1dc7dceb7859641088441ff9753&mpshare=1&scene=1&srcid=0112O4ivFbyChzTTH4vKU91t&sharer_sharetime=1646716025907&sharer_shareid=0ffc6ac03af605267e92344350efdb83&exportkey=AxNWDL6LKSsLK6MxSNkSF88%3D&acctmode=0&pass_ticket=TXKuWY6yeluRhUKTt0pk10ycuy%2BMsyJV6%2BXdxFjTtusuYyJMVPywg38icEXhKktM&wx_header=0#rd">[Slide&ÊûÅÂ∏ÇÂπ≥Âè∞Êé®ÈÄÅ]</a>
					  <a href="https://github.com/Xiaoqi-Zhao-DLUT/SSLSOD">[Code]</a><img
							  src="https://img.shields.io/github/stars/Xiaoqi-Zhao-DLUT/SSLSOD"/>
				 <p >   <strong>Xiaoqi Zhao</strong>, Youwei Pang, Lihe Zhang, Huchuan Lu, Xiang Ruan</p>
			   <p style="margin-top: -11px"><i>Thirty-Sixth AAAI Conference on Artificial Intelligence (<strong> AAAI </strong>), 2022. </i></p>
				</td>
			</tr>
			<tr></tr>
			<tr></tr>	
		
			<tr>
			
				<td width="206">
				<img src="./resources/paper_icon/ACMMM2021-Multisource.png" width="185px" height = "90" style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td> <a href="https://arxiv.org/pdf/2108.05076.pdf" target="_blank"> Multi-Source Fusion and Automatic Predictor Selection for Zero-Shot Video Object Segmentation</a>
					<a href="https://arxiv.org/pdf/2108.05076.pdf">[Paper]</a>
					<a href="https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247570266&idx=2&sn=547fbfdeaa02b0ff83dec3b24a58a632&chksm=ec1d3ea3db6ab7b5cfbe7ba2fffcd4460584f1e2743d8265562757e508030bf049d1c3140d3c&mpshare=1&scene=23&srcid=0902BUt8U9MY8k2IvY500f71&sharer_sharetime=1630565724193&sharer_shareid=5c55e87df338791d997e8905ad2ebfe0#rd">[Slide&ÊûÅÂ∏ÇÂπ≥Âè∞Êé®ÈÄÅ]</a>
					<a href="https://github.com/Xiaoqi-Zhao-DLUT/Multi-Source-APS-ZVOS">[Code]</a><img
							src="https://img.shields.io/github/stars/Xiaoqi-Zhao-DLUT/Multi-Source-APS-ZVOS"/>
				 <p > <strong>Xiaoqi Zhao</strong>, Youwei Pang, Jiaxing Yang, Lihe Zhang, Huchuan Lu</p>
			   <p style="margin-top: -11px"><i>ACM International Conference on Multimedia (<strong> ACM MM </strong>), 2021.
				<a  style="color:red;text-decoration:none;" ><strong>Oral</strong</a></i></p>
				</td>
			</tr>
			<tr></tr>
			<tr></tr>	

			<tr>
			
				<td width="206">
				<img src="./resources/paper_icon/MICCAI2021-MSNet.png" width="185px" height = "90" style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td> <a href="https://arxiv.org/pdf/2108.05082.pdf" target="_blank"> Automatic Polyp Segmentation via Multi-scale Subtraction Network</a>
					<a href="https://arxiv.org/pdf/2108.05082.pdf">[Paper]</a>
					<a href="https://github.com/Xiaoqi-Zhao-DLUT/MSNet">[Code]</a><img
							src="https://img.shields.io/github/stars/Xiaoqi-Zhao-DLUT/MSNet"/>  
				<p >  <strong>  <a  style="color:red;text-decoration:none;" >üî•Top 30 Most Cited MICCAI Papers in the Last 5 Years</a>  <a href="https://scholar.google.com/citations?hl=en&view_op=list_hcore&venue=QLpioUFGyGMJ.2024&vq=med_radiologymedicalimaging&cstart=20">[Link]</a>  </strong</p>
				<p <strong>Xiaoqi Zhao</strong>, Lihe Zhang, Huchuan Lu</p>
			   <p style="margin-top: -11px"><i>International Conference on Medical Image Computing and Computer Assisted Intervention (<strong> MICCAI </strong>), 2021.
				</td>
			</tr>
			<tr></tr>
			<tr></tr>	
		
			<tr>
			
				<td width="206">
				<img src="./resources/paper_icon/ECCV2020-GateNet.png" width="185px" height = "90" style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td> <a href="https://arxiv.org/pdf/2007.08074.pdf" target="_blank"> Suppress and Balance: A Simple Gated Network for Salient Object Detection</a>
					<a href="https://arxiv.org/pdf/2007.08074.pdf">[Paper]</a>
					<a href=" https://github.com/Xiaoqi-Zhao-DLUT/GateNet-RGB-Saliency/blob/master/2852.pdf">[Slide]</a>
					<a href="https://github.com/Xiaoqi-Zhao-DLUT/GateNet-RGB-Saliency">[Code]</a><img
							src="https://img.shields.io/github/stars/Xiaoqi-Zhao-DLUT/GateNet-RGB-Saliency"/>
<p >  <strong>  <a  style="color:red;text-decoration:none;" >üî•Top 50 Most Cited ECCV Papers (üèÜRanking Top 1 in Visual Saliency Field) in the Last 5 Years</a>  <a href="https://scholar.google.com/citations?hl=en&view_op=list_hcore&venue=cwIh2C-xo8kJ.2024&vq=eng_computervisionpatternrecognition&cstart=40">[Link]</a>  </strong</p>
				<p <strong>Xiaoqi Zhao</strong>*, Youwei Pang*, Lihe Zhang, Huchuan Lu, Lei Zhang</p>
			   <p style="margin-top: -11px"><i>European Conference on Computer Vision (<strong> ECCV </strong>), 2020.  <a  style="color:red;text-decoration:none;" ><strong>Oral</strong></a>
				</td>
			</tr>
			<tr></tr>
			<tr></tr>	
			<tr>
			
				<td width="206">
				<img src="./resources/paper_icon/ECCV2020-DANet.png" width="185px" height = "140" style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td> <a href="https://arxiv.org/pdf/2007.06811.pdf" target="_blank">A Single Stream Network for Robust and Real-time RGB-D Salient Object Detection</a>
				    <a href="https://arxiv.org/pdf/2007.06811.pdf">[Paper]</a>
                <a href="https://github.com/Xiaoqi-Zhao-DLUT/DANet-RGBD-Saliency">[Code]</a><img
                        src="https://img.shields.io/github/stars/Xiaoqi-Zhao-DLUT/DANet-RGBD-Saliency"/>
				 <p >  <strong>Xiaoqi Zhao</strong>,  Lihe Zhang, Youwei Pang, Huchuan Lu, Lei Zhang</p>
			   <p style="margin-top: -11px"><i>European Conference on Computer Vision (<strong> ECCV </strong>), 2020.
				</td>
			</tr>
			<tr></tr>
			<tr></tr>	

			<tr>
			
				<td width="206">
				<img src="./resources/paper_icon/CVPR2021-MINet.png" width="185px" height = "90" style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td> <a href="https://arxiv.org/pdf/2007.09062.pdf" target="_blank">Multi-scale Interactive Network for Salient Object Detection</a>
					[<a href="https://arxiv.org/pdf/2007.09062.pdf">Paper</a>]
					[<a href="https://github.com/lartpang/MINet">Code</a>]<img src="https://img.shields.io/github/stars/lartpang/MINet"/>
				 <p >       Youwei Pang*, <strong>Xiaoqi Zhao</strong>*, Lihe Zhang, Huchuan Lu</p>
			   <p style="margin-top: -11px"><i>IEEE Conference on Computer Vision and Pattern Recognition (<strong> CVPR </strong>), 2020.
				</td>
			</tr>
			<tr></tr>
			<tr></tr>	
	
			<tr>
			
				<td width="206">
				<img src="./resources/paper_icon/ECCV2020-HDFNet.png" width="185px" height = "90" style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td> <a href="https://arxiv.org/pdf/2007.06227.pdf" target="_blank">Hierarchical Dynamic Filtering Network for RGB-D Salient Object Detection</a>
					[<a href="https://arxiv.org/pdf/2007.06227.pdf">Paper</a>]
					[<a href="https://github.com/lartpang/HDFNet/blob/master/LongPresentation.pdf">Slide</a>]
					[<a href="https://github.com/lartpang/HDFNet">Code</a>]<img src="https://img.shields.io/github/stars/lartpang/HDFNet"/>
				 <p >     Youwei Pang, Lihe Zhang, <strong>Xiaoqi Zhao</strong>,  Huchuan Lu</p>
			   <p style="margin-top: -11px"><i>European Conference on Computer Vision (<strong> ECCV </strong>), 2020.
				</td>
			</tr>
			<tr></tr>
			<tr></tr>	
	
		</tbody>
		</table>
				
		<table id="tbPublications" width="100%">
		<tbody>	
		<h3 style="color: red">Journal Papers</h3>
		<tr>	
			<tr>
			
				<td width="206">
				<img src="./resources/paper_icon/IJCV-23-DBS.png" width="185px" height = "185" style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td> <a href="https://arxiv.org/pdf/2303.10396.pdf" target="_blank">Towards Diverse Binary Segmentation via A Simple yet General Gated Network</a>
					<a href="https://arxiv.org/pdf/2303.10396.pdf">[Paper]</a> 
					<a href="https://github.com/Xiaoqi-Zhao-DLUT/GateNet-RGB-Saliency">[Code]</a><img
							src="https://img.shields.io/github/stars/Xiaoqi-Zhao-DLUT/GateNet-RGB-Saliency"/>
				 <p >    <strong>Xiaoqi Zhao*</strong>, Youwei Pang*, Lihe Zhang, Huchuan Lu, Lei Zhang <br>
			   <p style="margin-top: -11px"><i>International Journal of Computer Vision (<strong> IJCV </strong>), 2024.
				</td>
			</tr>
			<tr></tr>
			<tr></tr>	

		<tr>	
			<tr>
			
				<td width="206">
				<img src="./resources/paper_icon/IJCV-23-AMP.png" width="185px" height = "140" style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td> <a href="https://arxiv.org/pdf/2303.10383.pdf" target="_blank">Adaptive Multi-source Predictor for Zero-shot Video Object Segmentation</a>
					<a href="https://arxiv.org/pdf/2303.10383.pdf">[Paper]</a> 
					<a href="https://github.com/Xiaoqi-Zhao-DLUT/Multi-Source-APS-ZVOS">[Code]</a><img
							  src="https://img.shields.io/github/stars/Xiaoqi-Zhao-DLUT/Multi-Source-APS-ZVOS"/>
				 <p >    <strong>Xiaoqi Zhao*</strong>, Shijie Chang*, Youwei Pang, Jiaxing Yang, Lihe Zhang, Huchuan Lu<br>
			   <p style="margin-top: -11px"><i>International Journal of Computer Vision (<strong> IJCV </strong>), 2024.
				</td>
			</tr>
			<tr></tr>
			<tr></tr>	
		
			
		<tr>	
			<tr>
			
				<td width="206">
				<img src="./resources/paper_icon/TPAMI24-ZoomNeXt.png" width="185px" height = "140" style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td> <a href="https://arxiv.org/pdf/2310.20208.pdf" target="_blank">ZoomNeXt: A Unified Collaborative Pyramid Network for Camouflaged Object Detection</a>
					<a href="https://arxiv.org/pdf/2310.20208.pdf">[Paper]</a>    <br>
					<a href="https://github.com/lartpang/ZoomNeXt">[Code]</a><img
							  src="https://img.shields.io/github/stars/lartpang/ZoomNeXt"/>
				 <p >     Youwei Pang*, <strong>Xiaoqi Zhao*</strong>, Tian-Zhu Xiang, Lihe Zhang, Huchuan Lu<br>
			   <p style="margin-top: -11px"><i>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong> TPAMI </strong>), 2024.
				</td>
			</tr>
			<tr></tr>
			<tr></tr>	
		
			<tr>	
				<tr>
				
					<td width="206">
					<img src="./resources/paper_icon/PAMI_Comptr.png" width="185px" height = "140" style="box-shadow: 4px 4px 8px #888">
					</td>		
					<td> <a href="https://arxiv.org/pdf/2307.12349.pdf" target="_blank">ComPtr: Towards Diverse Bi-source Dense Prediction Tasks via A Simple yet General Complementary Transformer</a>
						<a href="https://arxiv.org/pdf/2307.12349.pdf">[Paper]</a>    <br>
					 <p >   Youwei Pang*, <strong>Xiaoqi Zhao*</strong>, Lihe Zhang, Huchuan Lu<br>
				   <p style="margin-top: -11px"><i>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong> TPAMI </strong>), 2024  (under review).
					</td>
				</tr>
				<tr></tr>
				<tr></tr>	

				<tr>	
					<tr>
					
						<td width="206">
						<img src="./resources/paper_icon/TIP2022-MMFT.jpg" width="185px" height = "140" style="box-shadow: 4px 4px 8px #888">
						</td>		
						<td> <a href="https://arxiv.org/pdf/2203.04895v2.pdf" target="_blank">Joint Learning of Salient Object Detection, Depth Estimation and Contour Extraction</a>
							<a href="https://arxiv.org/pdf/2203.04895v2.pdf">[Paper]</a>
							<a href="https://github.com/Xiaoqi-Zhao-DLUT/MMFT">[Code]</a><img
									src="https://img.shields.io/github/stars/Xiaoqi-Zhao-DLUT/MMFT"/>
									<br>
						 <p >  <strong>Xiaoqi Zhao</strong>, Youwei Pang, Lihe Zhang, Huchuan Lu<br>
					   <p style="margin-top: -11px"><i>IEEE Transactions on Image Processing (<strong> TIP </strong>), 2022.
						</td>
					</tr>
					<tr></tr>
					<tr></tr>	

					<tr>	
						<tr>
						
							<td width="206">
							<img src="./resources/paper_icon/TIP23-CAVER.png" width="185px" height = "140" style="box-shadow: 4px 4px 8px #888">
							</td>		
							<td> <a href="https://arxiv.org/pdf/2112.02363.pdf" target="_blank">CAVER: Cross-Modal View-Mixed Transformer for Bi-Modal Salient Object Detection</a>
								<a href="https://arxiv.org/pdf/2112.02363.pdf">[Paper]</a>
								<a href="https://github.com/lartpang/CAVER">[Code]</a><img
										src="https://img.shields.io/github/stars/lartpang/CAVER"/>
										<br>
							 <p >    Youwei Pang, <strong>Xiaoqi Zhao</strong>, Lihe Zhang, Huchuan Lu<br>
						   <p style="margin-top: -11px"><i>IEEE Transactions on Image Processing (<strong> TIP </strong>), 2023.
  <p >  <strong>  <a  style="color:red;text-decoration:none;" >ESI Highly Cited Paper (1%)</a>  </strong</p>
 




							</td>
						</tr>
						<tr></tr>
						<tr></tr>
				<tr>	
						<tr>
						
							<td width="206">
							<img src="./resources/paper_icon/TMI-23-M2SNet.png" width="185px" height = "140" style="box-shadow: 4px 4px 8px #888">
							</td>		
							<td> <a href="https://arxiv.org/pdf/2303.10894.pdf" target="_blank">M2SNet: Multi-scale in Multi-scale Subtraction Network for Medical Image Segmentation</a>
								<a href="https://arxiv.org/pdf/2303.10894.pdf">[Paper]</a>  
									<a href="https://mp.weixin.qq.com/s/Hh7ZimtcvpOiMYU3FxBA3A">[Interpretation by CVHub (~5000 views)]</a>
								<a href="https://github.com/Xiaoqi-Zhao-DLUT/MSNet">[Code]</a><img
								src="https://img.shields.io/github/stars/Xiaoqi-Zhao-DLUT/MSNet"/>
										<br>
							 <p >     <strong>Xiaoqi Zhao</strong>, Hongpeng Jia, Youwei Pang, Long Lv, Feng Tian, Lihe Zhang, Weibing Sun, Huchuan Lu<br>
						   <p style="margin-top: -11px"><i>üèÜThe second place (2/100) in the MICCAI 2022 Challenge: Glaucoma Oct Analysis and Layer Segmentation (GOALS).
							</td>
						</tr>
						<tr></tr>
						<tr></tr>		
		
		</tbody>
		</table>





<h2><font> Academic Service </font></h2>
<ul>
    <li>
		<b>Journal Reviewer:</b></br>
	TPAMI, IJCV, TIP, TMI ...
    </li>

    <li>
        <b>Conference Reviewer:</b></br>
		CVPR: 2025, 2024, 2023, 2022 <strong>(Outstanding Reviewer ~2%[156/6247])</strong><br>
		ICCV: 2023, 2021 <br>
		ECCV: 2024, 2022 <br>
		AAAI: 2025, 2024, 2023, 2022 <br>
		NeurIPS: 2024, 2023 <br>
		ICLR: 2025, 2024 <br>
		ICML: 2024 <br>
    </li>
</ul>



<p><center><font>
        <br>&copy; Xiaoqi Zhao | Last updated: Mar. 2025 | </font></center>
</p>

</div>
</body></html>
<!-- <script type="text/javascript" src="//rf.revolvermaps.com/0/0/6.js?i=5267tz04tuz&amp;m=7&amp;c=e63100&amp;cr1=ffffff&amp;f=arial&amp;l=0&amp;bv=90&amp;lx=-420&amp;ly=420&amp;hi=20&amp;he=7&amp;hc=a8ddff&amp;rs=80" async="async"></script> -->
