<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Xiaoqi Zhao</title>
    <meta content="Xiaoqi zhao, https://implus.github.io" name="keywords">
    <style media="screen" type="text/css">
        html,
        body,
        div,
        span,
        applet,
        object,
        iframe,
        h1,
        h2,
        h3,
        h4,
        h5,
        h6,
        p,
        blockquote,
        pre,
        a,
        abbr,
        acronym,
        address,
        big,
        cite,
        code,
        del,
        dfn,
        em,
        font,
        img,
        ins,
        kbd,
        q,
        s,
        samp,
        small,
        strike,
        strong,
        sub,
        tt,
        var,
        dl,
        dt,
        dd,
        ol,
        ul,
        li,
        fieldset,
        form,
        label,
        legend,
        table,
        caption,
        tbody,
        tfoot,
        thead,
        tr,
        th,
        td {
            border: 0pt none;
            font-family: inherit;
            font-size: 100%;
            font-style: inherit;
            font-weight: inherit;
            margin: 0pt;
            outline-color: invert;
            outline-style: none;
            outline-width: 0pt;
            padding: 0pt;
            vertical-align: baseline;
        }

        a {
            color: #1772d0;
            text-decoration: none;
        }

        a:focus,
        a:hover {
            color: #f09228;
            text-decoration: none;
        }

        a.paper {
            font-weight: bold;
            font-size: 12pt;
        }

        b.paper {
            font-weight: bold;
            font-size: 12pt;
        }

        * {
            margin: 0pt;
            padding: 0pt;
        }

        body {
            position: relative;
            margin: 3em auto 2em auto;
            width: 800px;
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 14px;
            background: #eee;
        }

        h2 {
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 15pt;
            font-weight: 700;
        }

        h3 {
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 16px;
            font-weight: 700;
        }

        strong {
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 13px;
            font-weight: bold;
        }

        ul {
            list-style: circle;
        }

        img {
            border: none;
        }

        li {
            padding-bottom: 0.5em;
            margin-left: 1.4em;
        }

        alert {
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 13px;
            font-weight: bold;
            color: #FF0000;
        }

        em,
        i {
            font-style: italic;
        }

        div.section {
            clear: both;
            margin-bottom: 1.5em;
            background: #eee;
        }

        div.spanner {
            clear: both;
        }

        div.paper {
            clear: both;
            margin-top: 0.5em;
            margin-bottom: 1em;
            border: 1px solid #ddd;
            background: #fff;
            padding: 1em 1em 1em 1em;
        }

        div.paper div {
            padding-left: 230px;
        }

        img.paper {
            margin-bottom: 0.5em;
            float: left;
            width: 200px;
        }

        span.blurb {
            font-style: italic;
            display: block;
            margin-top: 0.75em;
            margin-bottom: 0.5em;
        }

        pre,
        code {
            font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
            margin: 1em 0;
            padding: 0;
        }

        div.paper pre {
            font-size: 0.9em;
        }
    </style>

    <link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet"
          type="text/css"/>
    <!-- Global site tag (gtag.js) - Google Analytics -->

</head>


<body>
<div style="margin-bottom: 1em; border: 1px solid #ddd; background-color: #fff; padding: 1em; height: 130px;">
    <div style="margin: 0px auto; width: 100%;">
        <img title="implus" style="float: left; padding-left: .01em; height: 130px;"
             src="./resources/images/zhaoxiaoqi.jpg">
        <div style="padding-left: 12em; vertical-align: top; height: 120px;">
            <span style="line-height: 150%; font-size: 20pt;">Xiaoqi Zhao (赵骁骐)</span><br>
            <span> <a href="https://www.dlut.edu.cn/">Dalian University of Technology (DLUT)</a></span><br>
            <span><strong>Address</strong>: No.2 Linggong Road, Ganjingzi District, Dalian City, Liaoning, China</span><br>
            <span><strong>Email</strong>: zxq[at]mail.dlut.edu.cn </span> <br>
        </div>
    </div>
</div>
<!--<div style="clear: both; background-color: #fff; margin-top: 1.5em; padding: .2em; padding-left: .3em;">-->

<div style="clear: both;">
    <div class="section">
        <h2>About Me [<a href="https://github.com/Xiaoqi-Zhao-DLUT">GitHub</a>]
            [<a href="https://scholar.google.com/citations?user=0EKcLI4AAAAJ&hl=zh-CN">Google Scholar</a>]
            <!--[<a href="./resources/cv/wwh_cv.pdf">CV</a>])-->
        </h2>
        <div class="paper">
        I received the B.E. degree in electronic and information engineering from the Dalian University of Technology (DLUT), Dalian, China in 2019. I am  currently pursuing the PhD degree in signal and information processing from  DLUT. My advisors are  <a href="http://faculty.dlut.edu.cn/lhzhang/zh_CN/index.htm">Prof. Lihe Zhang</a> and  <a href="http://faculty.dlut.edu.cn/Huchuan_Lu/zh_CN/index.htm">Prof. Huchuan Lu</a> from DLUT.            
 <br><br>
My research interests include RGB/RGB-D salient object detection, video object segmentation, camouflaged object detection, medical image segmentation (polyp), crowd counting and self-supervised learning.
       
            <br>
            <!-- <p style='color:red'><strong>I am looking for a postdoctoral position. Please feel free to contact me through the email.</strong></p> -->
        </div>
    </div>
</div>


<div style="clear: both;">
    <div class="section">
        <h2 id="confpapers">Honor</h2>
        <div class="paper">
            <ul>
                <li>
                    Seventh place of <a href="http://aiskyeye.com/leaderboard/">VisDrone 2021 Crowd Counting Challenge (ICCV 2021 Workshop)</a>, <strong>(7th from 77 teams)</strong>
                </li>
                
                <li>
                    Seventh place of 2020 Hualu Cup's first big data competition (Intelligent Diagnosis of Cancer Risk), <strong>5,000 RMB bonus (7th from 114 teams)</strong>
                </li>
                <li>
                    2020 Huawei Fellowship
                </li>
                 <li>
                    Third place of 2018 OPPO Top AI Competition (Portrait Segmentation), <strong>50,000 RMB bonus (3rd from 456 teams)</strong> 
                </li>
            </ul>
            <div class="spanner"></div>
        </div>
    </div>
</div>

<div style="clear: both;">
    <div class="section">
        <h2 id="experience">News</h2>
        <div class="paper">
            <ul>
                <li> 1 paper accepted in ACMMM 2021.  </li>
                <li> 1 paper accepted in MICCAI 2021.  </li>
                <li> 3 paper accepted in ECCV 2020 (1 oral).  </li>
                <li> 1 paper accepted in CVPR 2020.  </li>
            </ul>
            <div class="spanner"></div>
        </div>
    </div>
</div>
    


<div style="clear: both;">
    <div class="section">
        <h2 id="confpapers">Publications</h2>
 
   <div class="paper"><img class="paper" src="./resources/paper_icon/2021-SSL.png"
                                title="Self-Supervised Representation Learning for RGB-D Salient Object Detection">
            <div><strong>Self-Supervised Representation Learning for RGB-D Salient Object Detection</strong><br>
                <strong>Xiaoqi Zhao</strong>, Youwei Pang, Lihe Zhang, Huchuan Lu, Xiang Ruan<br>
                in arXiv:2101.12482 (2021)<br>
                <a href="https://arxiv.org/pdf/2101.12482.pdf">[Paper]</a>
                        <br>
                <alert> In this work, we propose a novel self-supervised learning (SSL) scheme to accomplish effective pre-training for RGB-D  SOD  without requiring  human  annotation. As the first method of SSL in RGB-D SOD, it can be taken as a new baseline for future research.
                </alert>
            </div>
            <div class="spanner"></div>
        </div>


 <div class="paper"><img class="paper" src="./resources/paper_icon/ACMMM2021-Multisource.png"
                                title="Automatic Polyp Segmentation via　 Multi-scale Subtraction Network">
            <div><strong>Multi-Source Fusion and Automatic Predictor Selection for Zero-Shot Video Object Segmentation</strong><br>
                <strong>Xiaoqi Zhao</strong>, Youwei Pang, Jiaxing Yang, Lihe Zhang, Huchuan Lu<br>
     in ACMMM 2021 <br>
                
                <br>
                <alert> In this paper, we propose a novel multi-source fusion network to effectively utilize the complementary features from the RGB, depth, static saliency and optical flow for zero-shot video object segmentation.  And, to get rid of the inevitable interference caused by low-quality optical flow, we design a novel predictor selection network, which automatically chooses the results from static saliency predictor and moving object predictor. 
                </alert>
            </div>
            <div class="spanner"></div>
        </div>
        
 <div class="paper"><img class="paper" src="./resources/paper_icon/MICCAI2021-MSNet.png"
                                title="Automatic Polyp Segmentation via　 Multi-scale Subtraction Network">
            <div><strong>Automatic Polyp Segmentation via Multi-scale Subtraction Network</strong><br>
                <strong>Xiaoqi Zhao</strong>,  Lihe Zhang, Huchuan Lu<br>
     in MICCAI 2021 <br>
                <a href="">[Paper]</a>
                <a href="https://github.com/Xiaoqi-Zhao-DLUT/MSNet">[Code]</a><img
                        src="https://img.shields.io/github/stars/Xiaoqi-Zhao-DLUT/MSNet"/>
                <br>
                <alert> In this paper, we present a novel multi-scale subtraction network (MSNet) to automatically segment polyps from colonoscopy images. MSNet runs at the fastest speed of ∼70fps among the existing polyp segmentation methods.
                </alert>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/paper_icon/ECCV2020-GateNet.png"
                                title="Suppress and Balance: A Simple Gated Network for Salient Object Detection">
            <div><strong>Suppress and Balance: A Simple Gated Network for Salient Object Detection</strong><br>
                <strong>Xiaoqi Zhao</strong>*, Youwei Pang*, Lihe Zhang, Huchuan Lu, Lei Zhang<br>
                in ECCV 2020 (<alert>Oral
                </alert>)<br>
                <a href="https://arxiv.org/pdf/2007.08074.pdf">[Paper]</a>
                <a href="https://github.com/Xiaoqi-Zhao-DLUT/GateNet-RGB-Saliency">[Code]</a><img
                        src="https://img.shields.io/github/stars/Xiaoqi-Zhao-DLUT/GateNet-RGB-Saliency"/>
              
                <br>
                <alert> The gate unit is simple yet effective, therefore, a gated FPN network can be used as a new baseline for dense prediction tasks.
                </alert>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/paper_icon/ECCV2020-DANet.png"
                                title="A Single Stream Network for Robust and Real-time RGB-D Salient Object Detection">
            <div><strong>A Single Stream Network for Robust and Real-time RGB-D Salient Object Detection</strong><br>
               <strong>Xiaoqi Zhao</strong>,  Lihe Zhang, Youwei Pang, Huchuan Lu, Lei Zhang<br>
                in ECCV 2020 <br>
                <a href="https://arxiv.org/pdf/2007.06811.pdf">[Paper]</a>
                <a href="https://github.com/Xiaoqi-Zhao-DLUT/DANet-RGBD-Saliency">[Code]</a><img
                        src="https://img.shields.io/github/stars/Xiaoqi-Zhao-DLUT/DANet-RGBD-Saliency"/>
       
                <br>
                <alert> We build a single-stream network with the novel depth-enhanced dual attention for real-time (speed: 32FPS, model size: 106.7 MB with the VGG-16 backbone ) and robust rgb-d salient object detection. The more efficient way of using depth information to guide early fusion and ehance feature discrimination in decoder, which are different from existing two-stream methods  concentrate on the cross-modal fusion between the RGB stream and the depth stream.
                </alert>            </div>
            <div class="spanner"></div>
        </div>

 <div class="paper"><img class="paper" src="./resources/paper_icon/CVPR2021-MINet.png"
                                title="Multi-scale Interactive Network for Salient Object Detection">
            <div><strong>Multi-scale Interactive Network for Salient Object Detection</strong><br>
                 Youwei Pang*, <strong>Xiaoqi Zhao</strong>*, Lihe Zhang, Huchuan Lu<br>
                in CVPR 2020<br>
                [<a href="https://arxiv.org/pdf/2007.09062.pdf">Paper</a>]
                [<a href="https://github.com/lartpang/MINet">Code</a>]<img src="https://img.shields.io/github/stars/lartpang/MINet"/>
   
                <br>
                <alert>In  this  paper,  we  investigate  the  multi-scale  issue  to propose an effective and efficient network MINet with the transformation-interaction-fusion strategy, for salient object detection.
                </alert>
            </div>
            <div class="spanner"></div>
        </div>

<div class="paper"><img class="paper" src="./resources/paper_icon/ECCV2020-HDFNet.png"
                                title="Hierarchical Dynamic Filtering Network for RGB-D Salient Object Detection">
            <div><strong>Hierarchical Dynamic Filtering Network for RGB-D Salient Object Detection</strong><br>
                 Youwei Pang, Lihe Zhang, <strong>Xiaoqi Zhao</strong>,  Huchuan Lu<br>
                in ECCV 2020<br>
                [<a href="https://arxiv.org/pdf/2007.06227.pdf">Paper</a>]
                [<a href="https://github.com/lartpang/HDFNet">Code</a>]<img src="https://img.shields.io/github/stars/lartpang/HDFNet"/>
         
                <br>
                <alert>The proposed model (HDFNet)  generates  adaptive  filters  with different receptive field sizes through the dynamic dilated pyramid module. It can make full use of semantic cues from multi-modal mixed features to achieve multi-scale cross-modal guidance, thereby enhancing the representation capabilities of the decoder.
                 HDFNet is an important baseline of the
                [<a href="https://arxiv.org/pdf/2105.00690.pdf">winning solution</a>] in NTIRE 2021 (Depth Guided Image Relighting Challenge)
                hosted in CVPR 2021 workshop (winner: AICSNTU-MBNet team (Asustek Computer Inc & National Taiwan University)).
                </alert>
            </div>
            <div class="spanner"></div>
        </div>


    </div>
</div>



<div style="clear: both;">
    <div class="section">
        <h2>Review Services</h2>
        <div class="paper">
            <strong>Journal Reviewer</strong><br>
            TPAMI
            <br>

            <strong>Conference Reviewer</strong><br>
            ICCV: 2021<br>
            <br>
        </div>
    </div>
 <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=qF0yb8INQ6Byeh4ZPe-hmtAT1XxBhyDE0-o194UrGx8&cl=ffffff&w=a"></script>
</body>
</html>
